; Eggo "global" config

[dfs]
; The location of the result data sets in the targeted distributed file system
; (dfs). Currently supports S3 (s3n://), HDFS (hdfs://), and local (file://)
; targets. The specified URL will be the root of the eggo directory structure
; (see spec)
dfs_root_url: s3n://bdg-eggo-test

; Path to stage the raw input data on the target distributed fs
; raw data ends up in <dfs_raw_data_url>/<dataset_name>
dfs_raw_data_url: %(dfs_root_url)s/raw

; Path to store tmp/intermediate data in the target distributed fs
; tmp data ends up in <dfs_root_url>/<dfs_tmp_data_dir>/<dataset_name>/<random_id>
dfs_tmp_data_url: %(dfs_root_url)s/tmp


[execution]
; The context for execution. Possible values are spark_ec2, director, local.
; There are config sections for each of the possible values.
context: spark_ec2

; Random identifier that is generated on module load.  Do not set this manually
;random_id: <generated-on-load>


[versions]
eggo_fork: bigdatagenomics
eggo_branch: master

adam_fork: bigdatagenomics
adam_branch: master

maven: 3.2.5


[client_env]
; Can be overridden by setting SPARK_HOME env var
spark_home: $SPARK_HOME  ; hack: this var gets interpolated into shell cmds so
						 ; if the env var is set, it should fill it


[worker_env]
; directory on worker machines where we can write to, including staging
; temporary large data files; it will be created with mkdir -p
work_path: /mnt/eggo_work

; path on worker machines where this file is copied to
eggo_config_path: %(work_path)s/eggo.cfg

; path on worker machines to the luigi config file
luigi_config_path: %(work_path)s/luigi.cfg

; equiv to SPARK_HOME on the worker environment
spark_home: /root/spark

; should have a bin subdir where the hadoop binary is located
hadoop_home: /root/ephemeral-hdfs

; path to the hadoop streaming jar
streaming_jar: %(hadoop_home)s/contrib/streaming/hadoop-streaming-1.0.4.jar

; string compatible with the --master option to spark-submitthis string must
; evaluate correctly in a shell environment
spark_master: spark://$(curl http://169.254.169.254/latest/meta-data/public-hostname):7077
; spark_master: local[2]

; path on worker machines where adam is built/installed
adam_home: %(work_path)s/adam

; path on worker machines where the eggo repo is checked out
eggo_home: %(work_path)s/eggo


[aws]
; These can be set/overridden by setting corresponding local env vars (in ALL_CAPS)
;aws_access_key_id: <MY_ACCESS_KEY>
;aws_secret_access_key: <MY_SECRET_KEY>
;ec2_key_pair: <MY_KEY>
;ec2_private_key_file: <MY_KEY_FILE>


[spark_ec2]
; Spark EC2 scripts configuration
; Path to local Spark installation
region: us-east-1
; set this to a valid value to specify an availability zone:
availability_zone: us-east-1a
instance_type: r3.2xlarge
num_slaves: 2
user: root
stack_name: bdg-eggo-test


[director]
; Cloudera Director configuration
; TODO: add docs to these options
region: us-east-1
; set this to a valid value to specify an availability zone:
availability_zone:
launcher_instance_type: m3.large
launcher_ami: ami-a25415cb  ; RHEL 6.4 x86
cluster_ami: %(launcher_ami)s
num_workers: 3
stack_name: bdg-eggo-test
; the pointers to the director configs are executed relative to CWD (which may
; be the same as EGGO_HOME); set them to an absolute path if desired, or use
; "%(eggo_home)s" to access the EGGO_HOME env variable
cloudformation_template: conf/director/cfn-cloudera-us-east-1-public-subnet.template
director_conf_template: conf/director/aws.conf
