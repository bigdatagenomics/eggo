#
# Simple AWS Cloudera Director configuration file with automatic role assignments
# that works as expected if you use a single instance type for all cluster nodes
#

#
# Cluster name
#

name: C5-Simple-AWS

#
# Cloud provider configuration (credentials, region or zone and optional default image)
#

provider {
    type: aws

    #
    # Get AWS credentials from the OS environment
    # See http://docs.aws.amazon.com/general/latest/gr/aws-security-credentials.html
    #
    # If specifying the access keys directly and not through variables, make sure to enclose
    # them in double quotes.

    accessKeyId: "%(accessKeyId)s"
    secretAccessKey: "%(secretAccessKey)s"

    #
    # Whether to publish access keys. Default is false.
    # Publishing access keys to clients is necessary for those clients to have access to S3.
    # These keys are published in the configuration files of all HDFS clients.
    #

    publishAccessKeys: true

    #
    # ID of the Amazon AWS region to use
    # See: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html
    #

    region: %(region)s

    #
    # Region endpoint (if you are using one of the Gov. regions)
    #

    # regionEndpoint: ec2.us-gov-west-1.amazonaws.com

    #
    # Name of the public key registered in AWS
    # See: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html
    #

    keyName: %(keyName)s

    #
    # ID of the VPC subnet
    # See: http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html
    #

    subnetId: %(subnetId)s

    #
    # Comma separated list of security group IDs
    # See: http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_SecurityGroups.html
    #

    securityGroupsIds: %(securityGroupsIds)s

    #
    # A prefix that Cloudera Director should use when naming the instances (this is not part of the hostname)
    #

    instanceNamePrefix: cloudera-director

    #
    # Specify a size for the root volume (in GBs). Cloudera Director will automatically expand the
    # filesystem so that you can use all the available disk space for your application
    # See: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/storage_expand_partition.html
    #

    rootVolumeSizeGB: 100 # defaults to 50 GB if not specified

    #
    # Specify the type of the EBS volume used for the root partition. Defaults to gp2
    # See: http://aws.amazon.com/ebs/details/
    #

    # rootVolumeType: gp2 # OR standard (for EBS magnetic)

    #
    # Whether to associate a public IP address with instances or not. If this is false
    # we expect instances to be able to access the internet using a NAT instance
    #
    # Currently the only way to get optimal S3 data transfer performance is to assign
    # public IP addresses to your instances and not use NAT (public subnet type of setup)
    #
    # See: http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-ip-addressing.html
    #

    associatePublicIpAddresses: true

}

#
# SSH credentials to use to connect to the instances
#

ssh {
    username: ec2-user # for RHEL image
    privateKey: /home/ec2-user/id.pem # with an absolute path to .pem file
}

#
# A list of instance types to use for group of nodes or management services
#

instances {
    xl {
        type: d2.xlarge

        #
        # Amazon Machine Image (AMI)
        #
        # See: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html
        # Compatibility matrix: https://aws.amazon.com/amazon-linux-ami/instance-type-matrix/
        #
        # Red Hat Enterprise Linux AMI IDs: http://aws.amazon.com/partners/redhat/
        # We support Red Hat Enterprise Linux 6.4 (64bit) 64bit PV or HVM
        #

        image: %(image)s

        #
        # Name of the IAM Role to use for this instance type
        # http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html
        #

        # iamProfileName: REPLACE-ME

        tags {
            owner: %(keyName)s
        }

        bootstrapScript: """#!/bin/sh

# This is an embedded bootstrap script that runs as root and can be used to customize
# the instances immediately after boot and before any other Cloudera Director action

# If the exit code is not zero Cloudera Director will automatically retry

echo 'Hello World!'
exit 0

"""
    }
}

#
# Configuration for Cloudera Manager. Cloudera Director can use an existing instance
# or bootstrap everything from scratch for a new cluster
#

cloudera-manager {

    instance: ${instances.xl} {
        tags {
            application: "Cloudera Manager 5"
            group: manager
        }
    }

    #
    # Automatically activate 60-Day Cloudera Enterprise Trial
    #

    enableEnterpriseTrial: true

}

#
# Cluster description
#

cluster {

  # List the products and their versions that need to be installed.
  # These products must have a corresponding parcel in the parcelRepositories
  # configured above. The specified version will be used to find a suitable
  # parcel. Specifying a version that points to more than one parcel among
  # those available will result in a configuration error. Specify more granular
  # versions to avoid conflicts.

  products {
    CDH: 5
  }

  #
  # Optional override of CDH parcel repositories
  #

  # parcelRepositories: ["http://archive.cloudera.com/cdh5/parcels/latest/"]

  # OR for CDH4:
  # parcelRepositories: ["http://archive.cloudera.com/cdh4/parcels/latest/",
  #                      "http://archive.cloudera.com/impala/parcels/latest/",
  #                      "http://archive.cloudera.com/spark/parcels/latest/"]


  #
  # OR for CDH4
  #
  # products {
  #   CDH: 4
  #   IMPALA: 1.3
  #   SPARK: 0.9
  # }

  # NOTE: Sentry is only supported in Cloudera Manager versions 5.1 onward.
  services: [HDFS, YARN, ZOOKEEPER, IMPALA, HIVE, HUE, OOZIE, SPARK_ON_YARN]

  #
  # Optional custom service configurations
  #
  # configs {
  #     HDFS {
  #       dfs_block_size: 1342177280
  #     }
  #
  #     MAPREDUCE {
  #       mapred_system_dir: /user/home
  #       mr_user_to_impersonate: mapred1
  #     }
  # }

  #
  # Optional configuration for existing external database for Hive Metastore or Sentry databases
  #

  # databases {
  #     HIVE {
  #         type: postgresql
  #         host: db.example.com
  #         port: 123
  #         user: hive
  #         password: pass
  #         name: hive_db
  #     },
  #     SENTRY {
  #         type: postgresql
  #         host: db.example.com
  #         port: 123
  #         user: sentry
  #         password: pass
  #         name: sentry_db
  #     },
  # }

  #
  # Optional configuration for creating external database on the fly for Hive Metastore or Sentry
  # database
  #

  # databaseTemplates: {
  #     HIVE {
  #         name: hivetemplate
  #         databaseServerName: mysql1 # Must correspond to an external database server named above
  #         databaseNamePrefix: hivemetastore
  #         usernamePrefix: hive
  #     },
  #     SENTRY {
  #         name: sentrytemplate
  #         databaseServerName: mysql1
  #         databaseNamePrefix: sentrydb
  #         usernamePrefix: sentry
  #     },
  # }

  masters {
    count: 1

    instance: ${instances.xl} {
      tags {
        group: master
      }
    }

    roles {
      HDFS: ${roles.HDFS_MASTERS}
      YARN: ${roles.YARN_MASTERS}
      ZOOKEEPER: ${roles.ZOOKEEPER_MASTERS}
      IMPALA: ${roles.IMPALA_MASTERS}
      HIVE: ${roles.HIVE_MASTERS}
      HUE: ${roles.HUE_MASTERS}
      OOZIE: ${roles.OOZIE_MASTERS}
      SPARK_ON_YARN: [SPARK_YARN_HISTORY_SERVER]
    }

    # Optional custom role configurations
    # configs {
    #   HDFS {
    #      NAMENODE {
    #        dfs_name_dir_list: /data/nn
    #        namenode_port: 1234
    #      }
    #   }
    # }
  }

  workers {
    count: %(num_workers)s
    #
    # Minimum number of instances required to set up the cluster.
    # Fail and quit if minCount number of instances is not available in this cloud
    # environment. Else, continue setting up the cluster.
    #
    minCount: 2

    instance: ${instances.xl} {

      # Put all cluster nodes in a placement group for improved network performance
      # Note: this only works for a limited set of instances
      # http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html

      # placementGroup: REPLACE-ME

      tags {
        group: worker
      }
    }

    roles {
      HDFS: ${roles.HDFS_WORKERS}
      YARN: ${roles.YARN_WORKERS}
      IMPALA: ${roles.IMPALA_WORKERS}
    }

    # Optional custom role configurations

    # configs {
    #   HDFS {
    #     DATANODE {
    #        dfs_data_dir_list: /data/dn
    #      }
    #   }
    #   HBASE {
    #     REGIONSERVER {
    #       hbase_regionserver_java_heapsize: 4000000000
    #     }
    #   }
    # }
  }

  gateways {
    count: 1

    instance: ${instances.xl} {
      tags {
        group: gateway
      }
    }

    roles {
      HDFS: [GATEWAY]
      HIVE: [GATEWAY]
      SPARK_ON_YARN: [GATEWAY]
      YARN: [GATEWAY]
    }

    # Optional custom role configurations
    # configs {
    #   HIVE {
    #     GATEWAY {
    #       hive_metastore_timeout: 3000
    #       client_config_root_dir: /etc/hive
    #     }
    #   }
    # }
  }
}
