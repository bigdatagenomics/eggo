; Eggo "global" config

; If new sections are added, ensure that
; eggo.config.assert_eggo_config_complete is updated as well


[dfs]
; The location of the result data sets in the targeted distributed file system
; (dfs). Currently supports S3 (s3n://), HDFS (hdfs://), and local (file://)
; targets. The specified URL will be the root of the eggo directory structure
; (see spec)
dfs_root_url: s3n://bdg-eggo

; Path to stage the raw input data on the target distributed fs
; raw data ends up in <dfs_raw_data_url>/<dataset_name>
dfs_raw_data_url: %(dfs_root_url)s/raw

; Path to store tmp/intermediate data in the target distributed fs
; tmp data ends up in <dfs_root_url>/<dfs_tmp_data_dir>/<dataset_name>/<random_id>
dfs_tmp_data_url: %(dfs_root_url)s/tmp


[execution]
; The context for execution. Possible values are spark_ec2, director, local.
; There are config sections for each of the possible values.
context: spark_ec2

; Random identifier that is generated on module load.  Do not set this manually
; as it's generated-on-load
random_id:


[versions]
eggo_fork: bigdatagenomics
eggo_branch: master

adam_fork: bigdatagenomics
adam_branch: master

maven: 3.2.5


[client_env]
; Can be overridden by setting SPARK_HOME env var
spark_home: $SPARK_HOME  ; hack: this var gets interpolated into shell cmds so
						 ; if the env var is set, it should fill it


[worker_env]
; directory on worker machines where we can write to, including staging
; temporary large data files; it will be created with mkdir -p
work_path: /mnt/eggo_work

; path on worker machines where this file is copied to
eggo_config_path: %(work_path)s/eggo.cfg

; path on worker machines to the luigi config file
luigi_config_path: %(work_path)s/luigi.cfg

; equiv to SPARK_HOME on the worker environment
spark_home: /root/spark

; should have a bin subdir where the hadoop binary is located
hadoop_home: /root/ephemeral-hdfs

; path to the hadoop streaming jar
streaming_jar: %(hadoop_home)s/contrib/streaming/hadoop-streaming-1.0.4.jar

; string compatible with the --master option to spark-submitthis string must
; evaluate correctly in a shell environment
spark_master: spark://$(curl http://169.254.169.254/latest/meta-data/public-hostname):7077
; spark_master: local[2]

; path on worker machines where adam is built/installed
; last component of the path must be 'adam'
adam_home: %(work_path)s/adam

; path on worker machines where the eggo repo is checked out
; last component of the path must be 'eggo'
eggo_home: %(work_path)s/eggo

; memory allocated to spark on each slave. This is a configuration parameter for spark-submit
executor_memory: 55G


[aws]
; These can be set/overridden by setting corresponding local env vars (in
; ALL_CAPS)

; set these if using S3 as the underlying DFS
aws_access_key_id:
aws_secret_access_key:

; set these if using EC2 for the execution context
ec2_key_pair:
ec2_private_key_file:


[spark_ec2]
; Spark EC2 scripts configuration
; Path to local Spark installation
region: us-east-1
; set this to a valid value to specify an availability zone:
availability_zone:
instance_type: r3.2xlarge
spot_price:
num_slaves: 2
user: root
stack_name: bdg-eggo


[director]
; Cloudera Director configuration
; TODO: add docs to these options
region: us-east-1
; set this to a valid value to specify an availability zone:
availability_zone:
launcher_instance_type: m3.medium
launcher_ami: ami-00a11e68  ; RHEL-6.5_GA_HVM-20140929-x86_64-11-Hourly2-GP2
cluster_ami: %(launcher_ami)s
num_workers: 3
stack_name: bdg-eggo
user: ec2-user
; the pointers to the director configs are executed relative to CWD (which may
; be the same as EGGO_HOME); set them to an absolute path if desired, or use
; "%(eggo_home)s" to access the EGGO_HOME env variable
cloudformation_template: conf/director/cfn-cloudera-us-east-1-public-subnet.template
director_conf_template: conf/director/aws.conf
